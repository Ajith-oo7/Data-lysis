# ğŸ§¹ Comprehensive Data Cleaning Guide - Datalysis

This guide outlines all the data cleaning techniques available in the Datalysis application, following industry best practices.

## ğŸ“‹ Table of Contents

1. [Basic Data Cleaning (Always Applied)](#basic-data-cleaning)
2. [Enhanced Data Cleaning](#enhanced-data-cleaning)
3. [Advanced Features](#advanced-features)
4. [Configuration Options](#configuration-options)
5. [Usage Examples](#usage-examples)
6. [Cleaning Log & Audit Trail](#cleaning-log)

---

## ğŸ”§ Basic Data Cleaning (Always Applied)

These cleaning operations are automatically applied to all uploaded data:

### âœ… **Automatic Standardization**
```
â€¢ Empty strings ("") â†’ null
â€¢ Whitespace-only strings â†’ null  
â€¢ Common null representations ("N/A", "NA", "-", "null", "none", "undefined") â†’ null
â€¢ Currency values: $1,234.56 â†’ 1234.56
â€¢ Percentages: 75% â†’ 0.75
â€¢ Booleans: "true"/"yes"/"y"/"1" â†’ true, "false"/"no"/"n"/"0" â†’ false
â€¢ Date standardization to ISO format (YYYY-MM-DD)
â€¢ Whitespace trimming from all string values
```

### ğŸ”„ **Duplicate Removal**
```
â€¢ Exact duplicate rows removed automatically
â€¢ JSON-based comparison for detection
â€¢ Duplicate count logged and reported
```

---

## ğŸš€ Enhanced Data Cleaning

Activate enhanced cleaning by including `enhanced_cleaning` in your preprocessing rules. This enables all 15+ industry-standard techniques:

### ğŸ” **1. Advanced Missing Data Handling**

**Thresholds:**
- Remove columns with >50% missing values (configurable)
- Remove rows with >70% missing values (configurable)

**Smart Imputation:**
```python
# Numeric columns:
- Mean (for normal distributions)
- Median (for skewed distributions, auto-detected)
- KNN imputation (when enabled)

# Categorical columns:
- Mode (most frequent value)
- "Unknown" placeholder

# DateTime columns:
- Forward fill â†’ Backward fill
```

**Missing Indicator Variables:**
```python
# Create binary flags for missingness
original_column â†’ original_column_was_missing (0/1)
```

**Configuration:**
```
missing_threshold: 30%     # Column removal threshold
knn_imputation            # Enable KNN imputation
missing_indicators        # Create missing indicator variables
```

### ğŸ”§ **2. Intelligent Data Type Correction**

**Automatic Detection:**
```
â€¢ Text â†’ Numeric (if >80% values convertible)
â€¢ Text â†’ DateTime (if >50% values convertible)  
â€¢ Text â†’ Categorical (if <10% unique ratio or <20 unique values)
â€¢ Mixed data type handling
```

### ğŸ§¹ **3. Advanced Duplicate Removal**

**Features:**
```
â€¢ Exact duplicate detection
â€¢ Near-duplicate detection with fuzzy matching
â€¢ Text normalization for duplicate detection
```

### ğŸ§ª **4. Comprehensive Outlier Handling**

**Methods:**
```
IQR Method (default):
- Lower bound: Q1 - 1.5 Ã— IQR
- Upper bound: Q3 + 1.5 Ã— IQR

Z-Score Method:
- Values beyond Â±3 standard deviations
```

**Actions:**
```
â€¢ Cap: Limit outliers to bounds (default)
â€¢ Remove: Delete outlier rows
â€¢ Keep: Mark but preserve outliers
```

**Configuration:**
```
zscore_outliers          # Use Z-score method
remove_outliers          # Remove instead of capping
```

### ğŸ§± **5. Advanced Data Standardization & Normalization**

**Scaling Methods:**
```
Standard Scaling (Z-score):
- Mean = 0, Standard Deviation = 1

Min-Max Scaling:
- Range [0, 1]

Robust Scaling:
- Uses median and IQR (less sensitive to outliers)

Log Transform:
- log(1 + x) for skewed distributions
- Handles zero values automatically

Box-Cox Transform:
- Optimal power transformation for normality
- Automatic fallback to log transform
```

**Configuration:**
```
standardize_data         # Enable standardization
normalize_data minmax    # Use Min-Max scaling
log_transform           # Apply log transformation
boxcox                  # Apply Box-Cox transformation
```

### ğŸ”¡ **6. Advanced Text Cleaning**

**Operations:**
```
â€¢ Trim whitespace
â€¢ Case normalization (lowercase)
â€¢ HTML tag removal
â€¢ Emoji removal and standardization
â€¢ Special character removal
â€¢ Extra space consolidation
â€¢ URL removal
â€¢ Email address removal
â€¢ Punctuation removal
â€¢ Abbreviation standardization
â€¢ Common value standardization:
  - yes/y/true/1 â†’ "yes"
  - no/n/false/0 â†’ "no"
  - male/m â†’ "male"
  - female/f â†’ "female"
  - USA/United States/US â†’ "united states"
```

**Configuration:**
```
remove_html             # Remove HTML tags
remove_emojis           # Remove emoji characters
advanced_text_cleaning  # Enable all advanced features
```

### ğŸ§© **7. Data Integrity Validation**

**Logical Consistency:**
```
â€¢ Date logic: start_date < end_date
â€¢ Domain constraints:
  - Age: 0-120 years
  - Prices: â‰¥ 0
  - Custom constraints configurable
```

### ğŸ” **8. Format Standardization**

**Automatic Format Fixing:**
```
â€¢ Phone numbers: Extract digits only
â€¢ Email addresses: Lowercase normalization
â€¢ Currency fields: Remove symbols, convert to numeric
â€¢ Date formats: Standardize to ISO format
```

### ğŸ§® **9. Advanced Categorical Data Encoding**

**Encoding Methods:**
```
Label Encoding:
- Categories â†’ 0, 1, 2, 3...

One-Hot Encoding:
- Categories â†’ Binary columns
- Limited to â‰¤10 categories (configurable)

Target Encoding:
- Categories â†’ Target variable mean
- Requires target column specification

Frequency Encoding:
- Categories â†’ Count of occurrences
- Preserves frequency information
```

**Configuration:**
```
encode_categorical      # Enable encoding
label_encoding         # Use label encoding
target_encoding        # Use target encoding
frequency_encoding     # Use frequency encoding
```

### ğŸ§± **10. Binning and Grouping**

**Binning Methods:**
```
Equal-Width Bins:
- Uniform interval sizes

Equal-Frequency Bins:  
- Uniform sample sizes per bin

Custom Bins:
- User-defined breakpoints
```

**Configuration:**
```
create_bins             # Enable binning
binning age: 0,18,35,50,65,100  # Custom age groups
```

### ğŸ§® **11. Feature Engineering**

**Date Feature Extraction:**
```
â€¢ Year, Month, Day extraction
â€¢ Weekday extraction
â€¢ Date arithmetic (duration calculation)
```

**Text Feature Extraction:**
```
â€¢ Text length
â€¢ Word count
â€¢ Character analysis
```

**Configuration:**
```
feature_engineering     # Enable feature extraction
```

### ğŸ“Š **12. Aggregation & Transformation**

**Group Operations:**
```
â€¢ Group by categorical columns
â€¢ Calculate aggregates (sum, mean, count, etc.)
â€¢ Create derived metrics
```

### ğŸ§­ **13. Geospatial Data Cleaning**

**Validation:**
```
â€¢ Latitude: -90Â° to +90Â°
â€¢ Longitude: -180Â° to +180Â°
â€¢ Invalid coordinate removal
```

**Configuration:**
```
geospatial              # Enable geo validation
```

---

## ğŸ†• **Advanced Features**

### ğŸ”„ **14. Unit Conversion System**

**Automatic Conversions:**
```
Distance:
- Kilometers â†” Miles
- Meters â†” Feet  
- Centimeters â†” Inches

Weight:
- Kilograms â†” Pounds
- Grams â†” Ounces

Temperature:
- Celsius â†” Fahrenheit
- Celsius â†” Kelvin
- Fahrenheit â†” Celsius

Volume:
- Liters â†” Gallons
- Milliliters â†” Fluid Ounces
```

**Configuration:**
```
unit_conversion         # Enable unit conversions
convert_units          # Auto-detect and convert common units
```

**Custom Conversions:**
```python
unit_conversion_config: {
    'conversion_rules': {
        'distance_km': {
            'type': 'distance',
            'conversion': 'km_to_miles'
        }
    },
    'auto_detect_units': True
}
```

---

## âš™ï¸ Configuration Options

### **Basic Preprocessing Rules**
```
remove_empty_rows
remove_empty_columns  
trim_strings
convert_types
normalize_case:column_name
replace:column_name:old_value|new_value
```

### **Enhanced Cleaning Rules**
```
enhanced_cleaning           # Enable all enhanced features
standardize_data           # Data standardization
log_transform             # Apply log transformation
boxcox                    # Apply Box-Cox transformation
encode_categorical         # Categorical encoding
target_encoding           # Target-based encoding
frequency_encoding        # Frequency-based encoding
feature_engineering        # Feature extraction
missing_indicators        # Create missing value flags
knn_imputation            # Advanced imputation
zscore_outliers           # Z-score outlier detection
remove_outliers           # Remove outliers instead of capping
remove_html               # Remove HTML tags
remove_emojis             # Remove emoji characters
advanced_text_cleaning    # Advanced text processing
unit_conversion           # Enable unit conversions
missing_threshold: 25%    # Custom missing value threshold
geospatial               # Geospatial validation
create_bins              # Enable binning
```

---

## ğŸ“ Usage Examples

### **Basic Cleaning**
```
Preprocessing Instructions:
remove_empty_rows
convert_types
trim_strings
```

### **Advanced Cleaning**
```
Preprocessing Instructions:
enhanced_cleaning
standardize_data
encode_categorical
feature_engineering
knn_imputation
missing_threshold: 20%
remove_outliers
```

### **Comprehensive Cleaning (All Features)**
```
Preprocessing Instructions:
enhanced_cleaning
log_transform
target_encoding
frequency_encoding
missing_indicators
advanced_text_cleaning
remove_html
remove_emojis
unit_conversion
feature_engineering
geospatial
missing_threshold: 15%
```

### **Domain-Specific Cleaning**
```
Financial Data:
enhanced_cleaning
standardize_data minmax
remove_outliers
feature_engineering
unit_conversion

Healthcare Data:
enhanced_cleaning  
knn_imputation
zscore_outliers
geospatial
missing_threshold: 15%
missing_indicators

Text-Heavy Data:
enhanced_cleaning
encode_categorical frequency_encoding
feature_engineering
advanced_text_cleaning
remove_html
remove_emojis
```

---

## ğŸ“Š Cleaning Log & Audit Trail

The enhanced cleaning system provides comprehensive logging:

### **Operation Tracking**
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "operation": "remove_missing_columns", 
  "details": "Removed columns with >50% missing: ['old_column']",
  "rows_before": 1000,
  "rows_after": 1000,
  "rows_changed": 0
}
```

### **Quality Metrics**
```json
{
  "completeness_before": 0.85,
  "completeness_after": 0.98,
  "missing_value_reduction": 0.13,
  "data_consistency_score": 0.92
}
```

### **Summary Report**
```json
{
  "original_shape": [1000, 15],
  "final_shape": [987, 12],
  "rows_removed": 13,
  "columns_removed": 3,
  "cleaning_operations": 8,
  "processing_time": "2.34 seconds"
}
```

---

## âœ… Best Practices

### **ğŸ” Data Assessment First**
1. Upload your data
2. Review the initial data profile
3. Identify specific cleaning needs
4. Configure appropriate cleaning rules

### **ğŸ¯ Incremental Cleaning**
1. Start with basic cleaning
2. Add enhanced techniques as needed
3. Monitor data quality improvements
4. Iterate based on analysis results

### **ğŸ“‹ Documentation**
1. Keep track of cleaning operations
2. Document business rules and constraints
3. Review cleaning logs for audit trail
4. Validate results with domain experts

### **ğŸ”„ Validation**
1. Always review cleaned data samples
2. Check data quality metrics
3. Verify domain-specific constraints
4. Test downstream analysis validity

---

## ğŸ† Quality Assurance

The Datalysis cleaning system ensures:

- **âœ… Reproducibility**: Same rules = same results
- **âœ… Transparency**: Full operation logging  
- **âœ… Configurability**: Flexible rule system
- **âœ… Validation**: Quality metrics and checks
- **âœ… Scalability**: Handles large datasets efficiently
- **âœ… Extensibility**: Easy to add new techniques

Transform your raw data into analysis-ready datasets with confidence! ğŸš€ 